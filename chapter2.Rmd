---
title: "Chapter 2 - Statistical Learning Notes"
author: "William Surles"
output: 
 html_document:
  self_contained: yes
  theme: flatly
  highlight: tango
  toc: true
  toc_float: true
---

***
> This document was updated on **`r Sys.Date()`**.

***

## Intro

  - Welp. Lets just get through all the code examples.
  - I spend too long talking if I start so I'm just gonna do the code and I will
come back and add notes if I need to.
  - The book has the best notes. : )
  - Really, this doc is jut a reference for me to use later

```{r set-options, eval=T, echo=F, cache=F}

knitr::opts_chunk$set(eval=T, echo=T, cache=T, message=F, warning=F)

```


```{r}

library(stringr)
library(readr)
library(dplyr)
library(ggvis)
library(ISLR)

```


## 2.1 What is Statistical Learning?

In essence, statistical learning refers to a set of approaches for estimating $f$.

  - $f$ represents the *systematic* information that $X$ provides about $Y$. 
  - $Y$ is a quantitave response that is observed. $X$ is a predictor that we believe explains some of $Y$s value. 
  - We may have many predictors that we assume have a relationship with $Y$. 
  - There is also some random variability or error in $Y$ which we call $\epsilon$. 
  - $f$ is some fixed but unknown function of $X_1,...,X_p$, and $\epsilon$ is a random error term, which is independent of $X$ and has mean zero. 
  - We write this generally as ...

$$Y = f(X) + \epsilon $$
It may be easier to think of this with an example:
  
  - Say we are a consultant to an advertising firm and our task is to provide advice on how to improve sales of a particular product. 
  - There is an advertising data set consisting of sales of the product in 200 different markets, along with advertising budgets for three different media: TV, radio, and newspaper. 
  - Our client cannont directly manipulate the sales $Y$
  - Our goal is to come up with an accurate model $f$ that can be used to predict sales $Y$ based on the advertising budgets $X$. 
  - Then we can recommend how they can adjust $X$ to increase $Y$.
  - We will represent the budgets as $X_1$ for TV, $X_2$ for radio, etc. These are called predictors, indpependent variables, input variables, features, or just variables.
  - Sales $Y$ is called the output variable, response, or dependent variable. 

  
### 2.1.1 Why estimate $f$?

Two main reasons: *prediction* and *inference*

Prediction:

  - For example, suppose we have the characteristics $X$ of a patient's blood sample from a lab. $Y$ is a variable encoding the patient's risk for a severe adverse reaction to a particular drug. 
  - We seek to predict Y using X so we can say if its okay to give this patient the drug or not. 
  - Its important to note that our prediction $\widehat{Y}$ will never be exactly right. There is always some error. 
  - There are two kinds of error, *reducible error* and *irreducible error*. 
  - The reducible error 
    - This is what we can reduce if we improve the accuracy of our model, $\widehat{f}$. We try to find the best statistical learning technique to create the most accurate model to estimate $f$. 
  - irreducible error 
    - This is the natural variablity in the respose. Even if we have a perfect model for $f$, there will still be some error it it. $\epsilon$ cannont be predicted with $X$. The error can be made up of unmeasurable variation in the drug, say from manufacturing variation, or possibly an unmeasured variable that would help us predict $Y$ but we have not measured it in the blood, so we cannot use it. 
  - If our goal is prediction we don't really care about the make up of our model $\widehat{f}$ but just care that it results in an accurate prediction. In this case more complicated models that do not give an explaination for how each variable affects $Y$ can be used. We call these 'black box' models. Here, we are free to use more complicated model. 
  - A prediciton question would be: *Which individuals are likely to respond positively to a mailing advertisment based on their demographic information?*
  
Inference:
  
  - Sometimes we just want to know how the variables $X$ impact the response $Y$, not to predict $Y$. 
  - In this case we want to use models that allow us to explain how the variables $X$ affect the respose $Y$. A complicated black box model does not help us understand what is going on. Certain types of models are more usefule here. They may not be as accurate as others for predicting $Y$ but they explain how things impact $Y$ which is what is most important. We need to know the form of $\widehat{f}$. 
  - Here we may want to find the small set of predictors that are substantially associated with Y. We may want to know how each predictor affects $Y$. Can we use a linear model or do we need something more complex?
  - A good example of inference is the question: *What effect will changing the price of a product have on sales?*
  
This is the key take away:
  
> Depending on whether our ultimate goal is prediction, inference, or a combination of the two, different methods for estimating f may be appropriate. For example, linear models allow for relatively simple and interpretable inference, but may not yield as accurate predictions as some other approaches. In contrast, some of the highly non-linear approaches that we discuss in the later chapters of this book can potentially provide quite accurate predictions for Y , but this comes at the expense of a less interpretable model for which inference is more challenging.

### 2.1.2 How do we estimate $f$?

We use the data:

  - We will always assume that we have observed a set of $n$ different data points.
  - These observations are clled the the *training data* because we will use these data points to train (or teach) our model how to estimate $f$. 
  - We want to find a function $\hat{f}$ such that $Y \approx \hat{f}(X)$ for any observation ($X$,$Y$).
  - In general most statical learning methods for this task can be characterized as esither *parametric* of *non-parametric*
  
Parametric Methods

Non-Parametric Methods


### 2.1.3 The trade-off between prediction accuracy and model interpretability

### 2.1.4 Supervised versus unsupervised learning

### 2.1.5 Regression versus classification problems

## 2.2 Assessing Model Accuracy

### 2.2.1 Measuring the quality of fit

### 2.2.2 The bias-variance trade-off

### 2.2.3 The classification setting

## 2.3 Lab: Intro to R

### 2.3.1 Basic Commands

  - `c()` combines its inputs into a vector

```{r}

x <- c(1,3,2,5)
x

```

  - The `=` symbol works too

```{r}

y = c(1,4,3)
y

```

  - Use `?c` to get help with the `c()` or any function. 
  - The '+' function will add arrays by index (they must have the same length)
  - `?length` to see what the `length` function does. Or any function.

```{r}

x <- c(1,6,2)
y <- c(1,4,3)
length(x)
length(y)
x + y

```

  - `ls()` allows you to look at a list of all objects.
  - `rm()` is used to delete objects 
    - I rarely do this. 
  
```{r}
ls()
rm(x,y)
ls()

```

  - Remove all objects

```{r}
rm(list=ls())
```

  - `matrix()` will create a matrix
    - it needs 3 arguments, the data, the # of rows, and # of columns
    - by default it will create the matrix by columns, left to right

```{r}
x = matrix(data=c(1,2,3,4), nrow = 2, ncol = 2)
x
```

  - you don't need to give the names of the arguments
  - you can change all of the default arguments, for example build a matrix by row, rathter than by column
  - if you do not assign the matrix to a variable, it will just create it and print it to the screen
  
```{r}
x = matrix(c(1,2,3,4), 2, 2, byrow=T)
x
```

  - `sqrt()` will work on each element of the matrix
  - `x^2` will square each element of the matrix
  
```{r}
sqrt(x)
x^2
```

  - `rnorm` creates a vetor or random values sampled from the normal distribution
  - we get a different sample each time we call the function
  - we can change the values of the normal distribution. 
  - By default the mean is zero and sd is 1
  - `cor` will compute the correlation between two vectors
  
```{r}
x = rnorm(50)
y = x + rnorm(50, mean=50, sd = .1)
cor(x,y)
```

  - `set.seed` will allow you to create the exact same set of random numbers
  - this is great for teaching as it lets others get the same set of random numbers in the next functinos
  
```{r}
set.seed(1303)
rnorm(50)
```
  
  - `mean` calculates the mean of a vector of numbers
  - `var` calculates the variance of a vector of numbers
  
```{r}
set.seed(3)
y = rnorm(100)
mean(y)
var(y)

# calculate standard deviation
sqrt(var(y))
sd(y)
```

***

### 2.3.2 Graphics

  - I will probably use ggvis for all plots rather than base R. 
  - It is a big improvement on grammar and functionality
  - There may be some special R plots I may need to use occasionaly, but it seems to be rare.

```{r}
x = rnorm(100)
y = rnorm(100)
plot(x,y)
plot(x,y, 
     xlab = "this is the x-axis",
     ylab = "this is the y-axis",
     main = "Plot of X vs Y"
     )
```

  - similar plot but with ggvis
  
```{r}

data.frame(x=rnorm(100), y=rnorm(100)) %>% 
  ggvis(~x, ~y) %>%
  layer_points(fill := "blue")

```
  
  - you can save the output of an R plot 
  - you can use`pdf()` or `jpeg()`
  - `system` lets you can run a system command on the computer
    - here I make a separat folder to put images
    - `dev.off` says we are done creating the plot
  - these steps are useful to share the analysis results in an image in an automated way
    - once you have created an analysis and explained it, it may be useful to send updated results on a daily basis to a location. 
    - images are nice becasue they can easily be shared in an iframe or flow and be quickly viewed 
  
```{r}
system("mkdir images")
pdf("images/Figure.pdf")
plot(x,y,col="green")
dev.off()
```

  - `?seq` creates a sequence of numbers
  - There are many option to control length and step size
  
```{r}

x = seq(1:10)
x

## This is a shortcut
x = 1:10
x

x = seq(-pi, pi, length=20)
x

```

  - `contour` creates a contour plot of 3 dimensional data (like topographic map)
  - it takes an x, y, and z argument corresponding to x,y positions
  - there are many arguments to fine tune the plot
  
```{r}
y = x
f = outer(x, y, function(x,y) cos(y)/(1+x^2))
contour(x,y,f)

contour(x,y,f,nlevels=45, add=T)

fa = (f-t(f))/2
contour(x,y,fa, nlevels=15)

```

  - `image` works like contour, but it uses colors like a heatmap
  - `persp` creates a 3D plot. THeta and phi control the viewing angle
  
```{r}

image(x,y,fa)
persp(x,y,fa)
persp(x,y,fa, theta=30)
persp(x,y,fa, theta=30, phi=20)
persp(x,y,fa, theta=30, phi=70)
persp(x,y,fa, theta=30, phi=40)

```

***
### 2.3.3 Indexing Data
  - The book just covers matrices
  - It works similar for vectors, and dataframes.
  - I will mostly be working with dataframes so I added some exampoles here

```{r}
A = matrix(1:16,4,4)
A
A[2,3]
A[c(1,3), c(2,4)]
A[1:3,2:4]
A[1:2,]
A[,1:2]
A[1,]
A[-c(1,3),]
dim(A)
```

  - All of these work on a data frame also
  - dataframes can also be indexed by column names which reads a little cleaner
  
```{r}
df <- data.frame(a=1:4, b=5:8, c=9:12, d=13:16)
df
df[2,3]
df[c(1,3), c(2,4)]
df[1:3,2:4]
df[1:2,]
df[1:2] ## it defaults to the columns if you do not put in the ,
df[,1:2]
df[1,]
df[-c(1,3),]
dim(df)

df$a
df[c('a','b')]
df[1:2, c('a','b')]
df[c(1,4), c('a','d')]

```

***
### 2.3.4 Loading Data

  - I will probably only use readr since it is a newer library that replaces a lot of the code in this section
  - The `Auto` dataset is already part of the ISLR library so I don't need to load it
  - `fix` opens a spreadsheet viewer but its really slow
    - This has been replaced by the variable viewer built into R studio
  - `read_csv` is the ref function. It has many arguments to customize loading the data, but its defaults do a great job and make life easy. In most cases it does exactly what you would want. 
  
```{r}

## This just needs to be run the first time
# system("mkdir data")
# write_csv(Auto, "data/Auto.csv")

df_auto <- read_csv("data/Auto.csv")
glimpse(df_auto)
head(df_auto)
names(df_auto)

```

***
### 2.3.5 Aditional Graphical and Numerical Summaries

```{r}

plot(df_auto$cylinders, df_auto$mpg)

```

  - I typically use `ggvis` library for plots or `rcharts` if I want javascript charts. 
    - The base r plots are often used in old books like this but its worth learning the improved charting libraries.
  - I also never use `attach` because dplyr and ggvis let me reference the column name directly

```{r}

df_auto %>%
  ggvis(~cylinders, ~mpg) %>%
  layer_points(
    fill := "green",
    fillOpacity := .4
    )

```

  - `varwidth=T` makes the box width based on the number of data points in the group I think
    - Thats one way to add more info to the chart but it does look a little weird. 
    
```{r}

df_auto$cylinders <- as.factor(df_auto$cylinders)

plot(df_auto$cylinders, df_auto$mpg)
plot(df_auto$cylinders, df_auto$mpg, col = 'red', varwidth=T, horizontal=T)
plot(df_auto$cylinders, df_auto$mpg, col = 'red', varwidth=T, xlab="cylinders", ylab = "MPG")

```

  - Its still a good idea to make the cylnders variable a categorical variable rather than numerical. i.e. make it a factor. 
    - This lets the boxplot skip the 7 which has no values
    
```{r}

df_auto %>%
  ggvis(~cylinders, ~mpg) %>%
  layer_boxplots(
    width = .9,
    fill := "green",
    fillOpacity := .4
    )

```


```{r}

hist(df_auto$mpg)
hist(df_auto$mpg, col=2)
hist(df_auto$mpg, col=2, breaks=15)

```


```{r}

attach(df_auto)
pairs(Auto)
pairs(~ mpg + displacement + horsepower + weight + acceleration)

```

  - I'm having trouble getting the `identify` function to work 
    - I think the best way to do this is by adding a tooltip to the ggvis chart
    
```{r}

plot(horsepower, mpg)
identify(horsepower, mpg, name)

```
  
  - You can create a tooltip for the ggvis chart pretty easily
  - You just need a function that describes what to print and how
  - then you call that function in the `add_tooltip` function. 
  - There rae plenty of examples to follow
  - This only seems to work in an interactive context so essentially it needs to use the shiny app
  
```{r}

all_values <- function(x) {
  if(is.null(x)) return(NULL)
  str_c(names(x), ": ", format(x), collapse = '<br>')
}

df_auto %>%
  ggvis(~horsepower, ~mpg) %>%
  layer_points(
    fill := "green",
    fillOpacity := .6
  ) %>%
  add_tooltip(all_values, "hover")

```



```{r}

summary(df_auto)
summary(df_auto$mpg)
```

  - since all of the analysis here is reproducible with the code, I don't save the workspace. 
    - It might be nice to do this if there is data that took a long time to get, but in that case I would just save the data. It all pretty fast to load once on the local machine
  - I also don't use `savehistory()` or `loadhistory()` since I work in the editor or in this case in an r notebook. 
    - the author was using the r command line so this would have been helpful. 
    - would highly recommend using Rstudio and rmarkdown files (or notebooks) over using the r command line. 
    
### Thoughts

  - This section does introduce the reader to some interesting function in R.
  - Its clear that this was written decades ago.
  - R has come a long way with data analysis functions and I try to point to some of the more useful libraries here
  - I'm looking forward to getting to the statistical learning.

## 2.4 Exercises

```{r}

data(College)
head(College)
```